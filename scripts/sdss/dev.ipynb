{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import tarfile\n",
    "import glob\n",
    "import healpy as hp\n",
    "from astropy.io import fits\n",
    "from astropy.table import Table\n",
    "from multiprocessing import Pool\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import h5py\n",
    "\n",
    "URL = \"http://vipers.inaf.it/data/pdr2/spectra/\"\n",
    "SURVEYS = [\"VIPERS_W1_SPECTRA_1D_PDR2.tar.gz\", \"VIPERS_W4_SPECTRA_1D_PDR2.tar.gz\"]\n",
    "\n",
    "SURVEY_SAVE_DIRS = [\"vipers_w1\", \"vipers_w4\"]\n",
    "HEADER_KEYS = ['ID', 'RA', 'DEC', 'REDSHIFT', 'REDFLAG', 'EXPTIME', 'NORM', 'MAG']\n",
    "\n",
    "\n",
    "def download_data(vipers_data_path: str = ''):\n",
    "    \"\"\"Download the VIPERS data from the web and unpack it into the specified directory.\"\"\"\n",
    "    # Create the output directory if it does not exist\n",
    "    if not os.path.exists(vipers_data_path):\n",
    "        os.makedirs(vipers_data_path)\n",
    "\n",
    "    # Download each file\n",
    "    for file in SURVEYS:\n",
    "        local_path = os.path.join(vipers_data_path, file)\n",
    "        subdirectory_path = os.path.join(vipers_data_path, file.replace(\".tar.gz\", \"\"))\n",
    "\n",
    "        # Create a subdirectory for each file\n",
    "        if not os.path.exists(subdirectory_path):\n",
    "            os.makedirs(subdirectory_path)\n",
    "\n",
    "        # Check if file needs to be downloaded\n",
    "        if not os.path.exists(local_path):\n",
    "            print(f\"Downloading {file}...\")\n",
    "            response = requests.get(URL + file, stream=True)\n",
    "            if response.status_code == 200:\n",
    "                with open(local_path, 'wb') as f:\n",
    "                    for chunk in response.iter_content(chunk_size=8192):\n",
    "                        f.write(chunk)\n",
    "            else:\n",
    "                print(f\"Failed to download {file}. Status code: {response.status_code}\")\n",
    "                continue\n",
    "\n",
    "        # Unpack the tar.gz file into its specific subdirectory\n",
    "        print(f\"Unpacking into {subdirectory_path}...\")\n",
    "        with tarfile.open(local_path, \"r:gz\") as tar:\n",
    "            tar.extractall(path=subdirectory_path)\n",
    "        print(f\"Unpacked successfully!\\n\")\n",
    "\n",
    "        # Remove the tar files\n",
    "        os.remove(local_path)\n",
    "\n",
    "\n",
    "def extract_data(filename):\n",
    "    \"\"\"Extract the contents of a tar file to a dictionary for each file\"\"\"\n",
    "    hdu = fits.open(filename)\n",
    "    header = hdu[1].header\n",
    "    data = hdu[1].data\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    # Loop through the header keys and add them to the results dictionary\n",
    "    for key in HEADER_KEYS:\n",
    "        results[key] = float(header[key])\n",
    "    \n",
    "    # Add the spectrum data to the results dictionary\n",
    "    results['spectrum_flux'] = data['FLUXES'].astype(np.float32)\n",
    "    results['spectrum_wave'] = data['WAVES'].astype(np.float32)\n",
    "    results['spectrum_ivar'] = data['NOISE'].astype(np.float32)\n",
    "    results['spectrum_mask'] = data['MASK'].astype(np.float32)\n",
    "    \n",
    "    hdu.close()\n",
    "    return results\n",
    "\n",
    "\n",
    "def save_in_standard_format(results: Table, survey_subdir: str, nside: int):\n",
    "    \"\"\"Save the extracted data in a standard format for the given survey.\"\"\"\n",
    "    table = Table(results)\n",
    "\n",
    "    # Get keys\n",
    "    keys = table.keys()\n",
    "\n",
    "    # Get healpix files\n",
    "    healpix_indices = hp.ang2pix(nside, table['RA'], table['DEC'], lonlat=True, nest=True)\n",
    "    unique_indices = np.unique(healpix_indices)\n",
    "\n",
    "    for index in tqdm(unique_indices, desc=\"Processing HEALPix indices\"):\n",
    "        mask = healpix_indices == index\n",
    "        grouped_data = table[mask]\n",
    "        healpix_subdir = os.path.join(survey_subdir, f'healpix={index}')\n",
    "\n",
    "        if not os.path.exists(healpix_subdir):\n",
    "            os.makedirs(healpix_subdir)\n",
    "\n",
    "        output_path = os.path.join(healpix_subdir, '001-of-001.h5')\n",
    "\n",
    "        with h5py.File(output_path, 'w') as output_file:\n",
    "            for key in keys:\n",
    "                output_file.create_dataset(key.lower(), data=grouped_data[key])\n",
    "            output_file.create_dataset('object_id', data=grouped_data['ID'])\n",
    "            output_file.create_dataset('healpix', data=np.full(grouped_data['ID'].shape, index))\n",
    "\n",
    "\n",
    "def main(vipers_data_path: str = '', nside: int = 16, num_processes: int = 10):\n",
    "    \"\"\"\n",
    "    Download and extract the VIPERS spectra into a standard format using HEALPix indices.\n",
    "\n",
    "    Args:\n",
    "        vipers_data_path (str): The path to the directory where the VIPERS data is stored.\n",
    "        nside (int): The nside parameter for the HEALPix indexing.\n",
    "        num_processes (int): The number of parallel processes to run for extracting the data.\n",
    "    \"\"\"\n",
    "    # Download the VIPERS data if it does not exist\n",
    "    if not os.path.exists(vipers_data_path):\n",
    "        download_data(vipers_data_path)\n",
    "\n",
    "    for survey, survey_save_dir in zip(SURVEYS, SURVEY_SAVE_DIRS):\n",
    "        print(f\"Processing {survey}...\", flush=True)\n",
    "\n",
    "        # Load all fits file, standardize them and append to HDF5 file\n",
    "        survey = survey.replace(\".tar.gz\", \"\")\n",
    "        files = glob.glob(os.path.join(vipers_data_path, survey, '*.fits'))\n",
    "        files = files\n",
    "\n",
    "        # Run the parallel processing\n",
    "        with Pool(num_processes) as pool:\n",
    "            results = list(tqdm(pool.imap(extract_data, files), total=len(files)))\n",
    "\n",
    "        survey_save_dir = os.path.join(vipers_data_path, survey_save_dir)\n",
    "        if not os.path.exists(survey_save_dir):\n",
    "            os.makedirs(survey_save_dir)\n",
    "\n",
    "        save_in_standard_format(results, survey_save_dir, nside)\n",
    "        print(f\"Finished processing {survey}!\\n\")\n",
    "\n",
    "        os.remove(os.path.join(vipers_data_path, survey))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing VIPERS_W1_SPECTRA_1D_PDR2.tar.gz...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60528/60528 [00:31<00:00, 1934.79it/s]\n",
      "Processing HEALPix indices: 100%|██████████| 5/5 [00:00<00:00,  6.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing VIPERS_W4_SPECTRA_1D_PDR2.tar.gz...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 30979/30979 [00:39<00:00, 785.85it/s]\n",
      "Processing HEALPix indices: 100%|██████████| 3/3 [00:00<00:00,  7.86it/s]\n"
     ]
    }
   ],
   "source": [
    "main('/mnt/ceph/users/polymathic/AstroPile/vipers/', num_processes=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;34m'healpix=1189'\u001b[0m/  \u001b[01;34m'healpix=1190'\u001b[0m/  \u001b[01;34m'healpix=1191'\u001b[0m/\n"
     ]
    }
   ],
   "source": [
    "ls /mnt/ceph/users/polymathic/AstroPile/vipers/vipers_w4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vipers_w1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/mnt/home/lparker/Documents/AstroFoundationModel/AstroPile_prototype/scripts/sdss/dev.ipynb Cell 4\u001b[0m in \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bpolymathiclin030/mnt/home/lparker/Documents/AstroFoundationModel/AstroPile_prototype/scripts/sdss/dev.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m vipers_w1\n",
      "\u001b[0;31mNameError\u001b[0m: name 'vipers_w1' is not defined"
     ]
    }
   ],
   "source": [
    "vipers_w1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "astrokernel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
